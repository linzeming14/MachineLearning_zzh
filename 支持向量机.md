### 支持向量机 ——周志华西瓜书 笔记

1.  间隔与支持向量

   给点训练样本集$D=\{(x_1, y_2, ..., (x_m, y_m))\}, y\in \{-1, +1\}$， 分类学习最基本的想法就是基于训练集$D$在样本空间中找到一个划分超平面，将不同类别的样本分开。需要划分超平面对训练样本局部扰动的“容忍”性最好，最鲁棒的， 对未见示例的泛化能力 最强。

   ​    在样本空间中， 划分超平面可通过如下线性方程来描述：
   $$
   w^Tx+b=0 \tag{1}
   $$
   其中$w=(w_1, w_2, ...;w_d)$为法向量， 决定了超平面的方向， $b$为位移项， 决定了超平面与原点之间的距离。
   $$
   r=\frac{|w^Tx+b|}{||w||}\tag{2}
   $$
   ​	假设超平面$(w,b)$能将训练样本正确分类，即对于$(x_i, y_i) \in D$， 若$y_i = +1$, 则有$w^Tx_i +b> 0$; 若$y_i = -1$, 则有$w^Tx_i +b < 0$， 令
   $$
   w^Tx_i +b \geq +1, y_i=+1;\\
   w^Tx_i +b \leq -1, y_i = -1 \tag{3}
   $$
   如下图所示，距离超平面最近的几个训练样本点使式（3）的等号成立， 他们被称为“支持向量”， 两个异类支持向量到超平面的距离之和为
   $$
   \gamma = \frac{2}{||w||} \tag{4}
   $$
   它被称为“间隔”

![](6.2.png)

​					欲找到具有"最大间隔"的划分超平面，也就是要找到满足式（3）中约束的参数$w$和b， 使得$\gamma  $最大， 即
$$
max_{w, b}\frac{2}{||w||} \\
s.t.\ \  y_i(w^Tx_i+b)\geq 1,\  i =1,2,...,m.\tag{5}
$$
​					显然，为了最大化间隔， 仅需最大化$||w||^{-1}$, 这等价于最小化$||w||^2$, 于是上式可重写为
$$
min_{w, b} \frac{1}{2} ||w||^2\\
s.t. \ y_i(w^Tx_i +b) \geq 1, \ \ i=1, 2, ...,m\tag{6}
$$
​					**这就是支持向量机（Support Vector Machine , 简称SVM）**的基本型。

2. 对偶问题

   最大间隔划分超平面所对应的模型为：
   $$
   f(x) = w^Tx +b \tag{7}
   $$
   其中$w$和$b$是模型参数， 注意到式（6）本身是一个凸二次规划问题， 能直接用现成的优化计算包求解， 到我们可以有更高效的方法。

   ​	对式（6）使用拉格朗日乘子法可得到其"对偶问题"。具体来说， 对式（6）的每条约束添加拉格朗日乘子$\alpha_i \geq 0$, 则该问题的拉个朗日函数可写为： 
   $$
   L(w, b, \alpha) = \frac{1}{2}||w||^2+ \sum_{i=1}^m \alpha_i(1- y_i(w^Tx_i+b)) \tag{8}
   $$
   其中$\alpha = (\alpha_1; \alpha_2;...;\alpha_m)$, 令$L(w, b ,\alpha)$对$w$和$b$的偏导为零可得：
   $$
   w=\sum_{i=1}^m \alpha_i y_i x_i, \tag{9}
   $$
   

$$
0=\sum_{i=1}^m \alpha_i y_i \tag{10}
$$

将式（9）（10）代入式（8）中即可将$L(w, b, \alpha)$中的$w$和$b$消去， 再考虑式（10）的约束， 就可以得到式（6）的对偶问题
$$
max_{\alpha}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m \alpha_i \alpha_j y_i y_jx^T_i x_j\tag{11}\\
s.t. \ \sum_{i=1}^m \alpha_i y_i=0 \\
\alpha_i\geq 0 \ i=1,2,..., m.
$$
解出$\alpha$后， 求出$w$和$b$即可得到模型：
$$
f(x)=w^Tx+b\\
=\sum_{i=1}^m \alpha_i y_ix^T_i+b\tag{12}
$$
从对偶问题解出$\alpha_i$ 是式（8）中的拉格朗日乘子， 它恰好对应着训练样本$(x_i, y_i)$, 注意到式(6)中有不等式约束， 因此上述过程需满足**KKT(Karush-kuhn-Tucker)**条件， 既要求
$$
\alpha_i\geq 0\\
y_if(x_i)-1 \geq 0\\ \tag{13}
\alpha(y_if(x_i)-1)=0
$$
于是， 对任意样本$(x_i, y_i)$， 总有$\alpha_i=0$或者$y_if(x_i)=1$.若$\alpha =0$，则该样本将不会再式（12）的求和中出现， 也就不会对$f(x)$有任何影响。 若$\alpha_i>0$,则必有$y_if(x)=1$,所对应的样本点位于最大间隔边界上， 是一个支持向量， 这显示出支持向量机的一个重要的性质：训练完成后， 大部分的训练样本都不需要保留，最终模型仅与支持向量有关。

3. 核函数

   在现实任务中， 原始样本空间内也许并不存在一个能正确划分两类样本的超平面，即线性不可分。 我们可以将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

   ​	令$\phi(x)$表示将$x$映射后的特征向量， 于是在特征空间中划分超平面所对应的模型可表示为
   $$
   f(x) = w^T\phi(x) +b\tag{14}
   $$
   其中$w$和$b$是模型参数， 类似有
   $$
   min_{w, b} \frac{1}{2}||w||^3\tag{15} \\ 
   s.t. \ y_i(w^T\phi(x)+b) \geq 1 , i =1,2,...,m.
   $$
   其对偶问题是：
   $$
   max_{\alpha}\sum_{i=1}^m \alpha_i -\frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m \alpha_i \alpha_j y_i y_j \phi(x_i)^T\phi^(x_j)\tag{16} \\
   s.t. \sum_{i=1}^m \alpha_i y_i=0\\
   \alpha_i\geq0, i=1,2,..., m.
   $$
   

